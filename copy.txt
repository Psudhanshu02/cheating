import cv2
def compute_optical_flow_and_predict(video_path, interval=1):
    # Open the video file
    cap = cv2.VideoCapture(video_path)

    # Get the frames per second (Conv2dfps) of the video
    fps = cap.get(cv2.CAP_PROP_FPS)

    # Calculate the frame skip (number of frames to skip to get a frame every 10 seconds)
    frame_skip = int(fps * interval)

    # Initialize the previous frame
    ret, prev_frame = cap.read()
    prev_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    # List to store the predictions
    predictions = []

    frame_count = 0
    while cap.isOpened():
        # Read the next frame
        ret, frame = cap.read()

        if not ret:
            break

        frame_count += 1

        # If the frame count is a multiple of frame_skip, compute the optical flow
        if frame_count % frame_skip == 0:
            # Convert the frame to grayscale
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

            # Compute the optical flow
            flow = cv2.calcOpticalFlowFarneback(prev_frame, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

            # Convert flow to polar coordinates
            magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])

            # Create HSV image and populate it with the optical flow data
            hsv = np.zeros((gray.shape[0], gray.shape[1], 3), dtype=np.float32)
            hsv[..., 0] = angle * 180 / np.pi / 2
            hsv[..., 1] = 255
            hsv[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)

            # Convert HSV to int32's
            hsv = np.asarray(hsv, dtype= np.float32)
            rgb_flow = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)

            # Convert the optical flow image to a PyTorch tensor and add a batch dimension
            img_tensor = torch.from_numpy(rgb_flow).unsqueeze(0).permute(0, 3, 1, 2).float()
            
            conv1x1 = nn.Conv2d(img_tensor.shape[1], 2, 1)

            # Manually set the weights
            weights = torch.tensor([[[[0.5]], [[0.5]], [[0.5]]],  # for the first output channel
                                [[[0.5]], [[0.5]], [[0.5]]]])  # for the second output channel
            conv1x1.weight = nn.Parameter(weights)

            # Apply the convolution
            output = conv1x1(img_tensor)
            print(type(output))

            

            #Predict the speed
            with torch.no_grad():
                speed = model(output)

            # Append the speed prediction to the list
            predictions.append(speed.item())

            

            # Update the previous frame
            prev_frame = gray

    # Release the video capture
    cap.release()

    return predictions
